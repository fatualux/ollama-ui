[
  {
    "model": "llama2",
    "description": "The most popular model for general use."
  },
  {
    "model": "mistral",
    "description": "The 7B model released by Mistral AI, updated to version 0.2."
  },
  {
    "model": "llava",
    "description": "A novel end-to-end trained large multimodal model that combines a vision encoder and Vicuna for general-purpose visual and language understanding."
  },
  {
    "model": "mixtral",
    "description": "A high-quality Mixture of Experts (MoE) model with open weights by Mistral AI."
  },
  {
    "model": "starling-lm",
    "description": "Starling is a large language model trained by reinforcement learning from AI feedback focused on improving chatbot helpfulness."
  },
  {
    "model": "neural-chat",
    "description": "A fine-tuned model based on Mistral with good coverage of domain and language."
  },
  {
    "model": "codellama",
    "description": "A large language model that can use text prompts to generate and discuss code."
  },
  {
    "model": "dolphin-mixtral",
    "description": "An uncensored, fine-tuned model based on the Mixtral mixture of experts model that excels at coding tasks. Created by Eric Hartford."
  },
  {
    "model": "llama2-uncensored",
    "description": "Uncensored Llama 2 model by George Sung and Jarrad Hope."
  },
  {
    "model": "mistral-openorca",
    "description": "Mistral OpenOrca is a 7 billion parameter model, fine-tuned on top of the Mistral 7B model using the OpenOrca dataset."
  },
  {
    "model": "orca-mini",
    "description": "A general-purpose model ranging from 3 billion parameters to 70 billion, suitable for entry-level hardware."
  },
  {
    "model": "vicuna",
    "description": "General use chat model based on Llama and Llama 2 with 2K to 16K context sizes."
  },
  {
    "model": "wizard-vicuna-uncensored",
    "description": "Wizard Vicuna Uncensored is a 7B, 13B, and 30B parameter model based on Llama 2 uncensored by Eric Hartford."
  },
  {
    "model": "zephyr",
    "description": "Zephyr beta is a fine-tuned 7B version of mistral that was trained on a mix of publicly available, synthetic datasets."
  },
  {
    "model": "deepseek-coder",
    "description": "DeepSeek Coder is a capable coding model trained on two trillion code and natural language tokens."
  },
  {
    "model": "phind-codellama",
    "description": "Code generation model based on Code Llama."
  },
  {
    "model": "wizardcoder",
    "description": "Llama based code generation model focused on Python."
  },
  {
    "model": "dolphin-mistral",
    "description": "The uncensored Dolphin model based on Mistral that excels at coding tasks. Updated to version 2.6."
  },
  {
    "model": "llama2-chinese",
    "description": "Llama 2 based model fine-tuned to improve Chinese dialogue ability."
  },
  {
    "model": "nous-hermes",
    "description": "General use models based on Llama and Llama 2 from Nous Research."
  },
  {
    "model": "wizard-math",
    "description": "Model focused on math and logic problems."
  },
  {
    "model": "orca2",
    "description": "Orca 2 is built by Microsoft research, and are a fine-tuned version of Meta's Llama 2 models. The model is designed to excel particularly in reasoning."
  },
  {
    "model": "falcon",
    "description": "A large language model built by the Technology Innovation Institute (TII) for use in summarization, text generation, and chat bots."
  },
  {
    "model": "phi",
    "description": "Phi-2: a 2.7B language model by Microsoft Research that demonstrates outstanding reasoning and language understanding capabilities."
  },
  {
    "model": "openhermes",
    "description": "OpenHermes 2.5 is a 7B model fine-tuned by Teknium on Mistral with fully open datasets."
  },
  {
    "model": "codeup",
    "description": "Great code generation model based on Llama2."
  },
  {
    "model": "stable-beluga",
    "description": "Llama 2 based model fine-tuned on an Orca-style dataset. Originally called Free Willy."
  },
  {
    "model": "openchat",
    "description": "A family of open-source models trained on a wide variety of data, surpassing ChatGPT on various benchmarks. Updated to version 3.5-1210."
  },
  {
    "model": "everythinglm",
    "description": "Uncensored Llama2 based model with support for a 16K context window."
  },
  {
    "model": "medllama2",
    "description": "Fine-tuned Llama 2 model to answer medical questions based on an open source medical dataset."
  },
  {
    "model": "wizardlm-uncensored",
    "description": "Uncensored version of Wizard LM model."
  },
  {
    "model": "starcoder",
    "description": "StarCoder is a code generation model trained on 80+ programming languages."
  },
  {
    "model": "yi",
    "description": "A high-performing, bilingual language model."
  },
  {
    "model": "wizard-vicuna",
    "description": "Wizard Vicuna is a 13B parameter model based on Llama 2 trained by MelodysDreamj."
  },
  {
    "model": "bakllava",
    "description": "BakLLaVA is a multimodal model consisting of the Mistral 7B base model augmented with the LLaVA architecture."
  },
  {
    "model": "yarn-mistral",
    "description": "An extension of Mistral to support context windows of 64K or 128K."
  },
  {
    "model": "open-orca-platypus2",
    "description": "Merge of the Open Orca OpenChat model and the Garage-bAInd Platypus 2 model. Designed for chat and code generation."
  },
  {
    "model": "samantha-mistral",
    "description": "A companion assistant trained in philosophy, psychology, and personal relationships. Based on Mistral."
  },
  {
    "model": "sqlcoder",
    "description": "SQLCoder is a code completion model fined-tuned on StarCoder for SQL generation tasks."
  },
  {
    "model": "solar",
    "description": "A compact, yet powerful 10.7B large language model designed for single-turn conversation."
  },
  {
    "model": "meditron",
    "description": "Open-source medical large language model adapted from Llama 2 to the medical domain."
  },
  {
    "model": "stablelm-zephyr",
    "description": "A lightweight chat model allowing accurate, and responsive output without requiring high-end hardware."
  },
  {
    "model": "yarn-llama2",
    "description": "An extension of Llama 2 that supports a context of up to 128k tokens."
  },
  {
    "model": "deepseek-llm",
    "description": "An advanced language model crafted with 2 trillion bilingual tokens."
  },
  {
    "model": "magicoder",
    "description": "Magicoder is a family of 7B parameter models trained on 75K synthetic instruction data using OSS-Instruct, a novel approach to enlightening LLMs with open-source code snippets."
  },
  {
    "model": "dolphin-phi",
    "description": "2.7B uncensored Dolphin model by Eric Hartford, based on the Phi language model by Microsoft Research."
  },
  {
    "model": "codebooga",
    "description": "A high-performing code instruct model created by merging two existing code models."
  },
  {
    "model": "mistrallite",
    "description": "MistralLite is a fine-tuned model based on Mistral with enhanced capabilities of processing long contexts."
  },
  {
    "model": "wizardlm",
    "description": "General use 70 billion parameter model based on Llama 2."
  },
  {
    "model": "tinyllama",
    "description": "The TinyLlama project is an open endeavor to train a compact 1.1B Llama model on 3 trillion tokens."
  },
  {
    "model": "goliath",
    "description": "A language model created by combining two fine-tuned Llama 2 70B models into one."
  },
  {
    "model": "nexusraven",
    "description": "Nexus Raven is a 13B instruction tuned model for function calling tasks."
  },
  {
    "model": "alfred",
    "description": "A robust conversational model designed to be used for both chat and instruct use cases."
  },
  {
    "model": "nous-hermes2",
    "description": "The powerful family of models by Nous Research that excels at scientific discussion and coding tasks."
  },
  {
    "model": "xwinlm",
    "description": "Conversational model based on Llama 2 that performs competitively on various benchmarks."
  },
  {
    "model": "notux",
    "description": "A top-performing mixture of experts model, fine-tuned with high-quality data."
  },
  {
    "model": "notus",
    "description": "A 7B chat model fine-tuned with high-quality data and based on Zephyr."
  }
]
